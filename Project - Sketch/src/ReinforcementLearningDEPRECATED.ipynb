{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cet avion peut voler pendant  52438  s, soit  873  min, soit 2039  km Ã  une vitesse moyenne de  140  km/h et sa vitesse max sera de  200  km/h\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Roaming\\Python\\Python38\\site-packages\\seaborn\\rcmod.py:82: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if LooseVersion(mpl.__version__) >= \"3.0\":\n",
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\setuptools\\_distutils\\version.py:346: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  other = LooseVersion(other)\n",
      "C:\\Users\\User\\AppData\\Roaming\\Python\\Python38\\site-packages\\flatbuffers\\compat.py:19: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  import imp\n",
      "C:\\Users\\User\\AppData\\Roaming\\Python\\Python38\\site-packages\\keras\\utils\\image_utils.py:36: DeprecationWarning: NEAREST is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.NEAREST or Dither.NONE instead.\n",
      "  'nearest': pil_image.NEAREST,\n",
      "C:\\Users\\User\\AppData\\Roaming\\Python\\Python38\\site-packages\\keras\\utils\\image_utils.py:37: DeprecationWarning: BILINEAR is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BILINEAR instead.\n",
      "  'bilinear': pil_image.BILINEAR,\n",
      "C:\\Users\\User\\AppData\\Roaming\\Python\\Python38\\site-packages\\keras\\utils\\image_utils.py:38: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n",
      "  'bicubic': pil_image.BICUBIC,\n",
      "C:\\Users\\User\\AppData\\Roaming\\Python\\Python38\\site-packages\\keras\\utils\\image_utils.py:39: DeprecationWarning: HAMMING is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.HAMMING instead.\n",
      "  'hamming': pil_image.HAMMING,\n",
      "C:\\Users\\User\\AppData\\Roaming\\Python\\Python38\\site-packages\\keras\\utils\\image_utils.py:40: DeprecationWarning: BOX is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BOX instead.\n",
      "  'box': pil_image.BOX,\n",
      "C:\\Users\\User\\AppData\\Roaming\\Python\\Python38\\site-packages\\keras\\utils\\image_utils.py:41: DeprecationWarning: LANCZOS is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.LANCZOS instead.\n",
      "  'lanczos': pil_image.LANCZOS,\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.pyplot._IonContext at 0x1a3ddeb7460>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from functools import reduce\n",
    "from itertools import count\n",
    "from gymenv.PlaytimeEnv import PlaytimeEnv\n",
    "from gymenv.action_manager import ActionModel\n",
    "from objects.Maneuver import Mission_Maneuver\n",
    "from objects.AllManeuvers import LIST_MAN\n",
    "from objects.Plane import ULM\n",
    "from function.tools import *\n",
    "from function.j_methods import * \n",
    "import tensorflow as tf\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from collections import namedtuple, deque, Counter\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "plt.ion()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_list = {\n",
    "    'Plane': [ULM],\n",
    "    'GoalDistance': [0, 10, 15, 20, 30],\n",
    "    'RtBDistance': [0, 10, 15, 20, 30],\n",
    "    'Fuel': [1, 2, 3, 3, 4, 10],\n",
    "    # Start with just 2 states : good (sunny, no clouds) and bad (cloudy or rainy)\n",
    "    'Meteo': [\"Sunny\", \"Cloudy\", \"Misty\"],\n",
    "    'MissionType': [Mission_Maneuver.SCAR],  # , Mission_Maneuver.CAS\n",
    "    # Add ennemies number afterwards, weaponry\n",
    "    'Strength': ['Weak', 'Equal', 'Strong'],\n",
    "    'TimeMin': [0, 300],\n",
    "    'SynchroTime': [0, 1000],\n",
    "}\n",
    "combinations = get_all_combinations(param_list)\n",
    "first_c = list(filter(lambda c : c['TimeMin'] == 300 and c['Fuel'] == 3, combinations))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([],maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Save a transition\"\"\"\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RESET ENV\n",
      "{'altitude': 23373, 'distance': 18, 'gap': 4, 'length': 27, 'maneuver': 0, 'radius': 4, 'speed': 124, 'width': 15}\n",
      "['altitude', 'distance', 'gap', 'length', 'maneuver', 'radius', 'speed', 'width']\n",
      "7\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "env = PlaytimeEnv(combinations)\n",
    "env.reset()\n",
    "a = env.action_space.sample()\n",
    "# actions_number = reduce(lambda a,b: a*b, env.action_space.values())\n",
    "\n",
    "# print(actions_number)\n",
    "print({key : value.sample() for key, value in env.action_space.items()})\n",
    "nb_param_out = max(man._nb_param_() for man in LIST_MAN) + 1\n",
    "ordered_keys = [key for key in env.action_space.keys()]\n",
    "print(ordered_keys)\n",
    "BATCH_SIZE = 128\n",
    "GAMMA = 0.999\n",
    "EPS_START = 0.9\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY = 200\n",
    "TARGET_UPDATE = 10\n",
    "\n",
    "policy_net = ActionModel(len(env.state), nb_param_out)\n",
    "target_net  = ActionModel(len(env.state), nb_param_out)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()\n",
    "\n",
    "optimizer = optim.RMSprop(policy_net.parameters())\n",
    "memory = ReplayMemory(10000)\n",
    "steps_done = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\utils\\tensorboard\\__init__.py:4: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if not hasattr(tensorboard, \"__version__\") or LooseVersion(\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "The algorithm only supports (<class 'gym.spaces.box.Box'>, <class 'gym.spaces.discrete.Discrete'>, <class 'gym.spaces.multi_discrete.MultiDiscrete'>, <class 'gym.spaces.multi_binary.MultiBinary'>) as action spaces but Dict(altitude: Discrete(29700, start=300), distance: Discrete(30, start=15), gap: Discrete(5, start=1), length: Discrete(30, start=15), maneuver: Discrete(4), radius: Discrete(3, start=2), speed: Discrete(100, start=100), width: Discrete(30, start=15)) was provided",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32md:\\Playtime-sketch\\Project - Sketch\\src\\ReinforcementLearning.ipynb Cellule 5\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Playtime-sketch/Project%20-%20Sketch/src/ReinforcementLearning.ipynb#ch0000018?line=5'>6</a>\u001b[0m \u001b[39m# Parallel environments\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Playtime-sketch/Project%20-%20Sketch/src/ReinforcementLearning.ipynb#ch0000018?line=6'>7</a>\u001b[0m env \u001b[39m=\u001b[39m PlaytimeEnv(first_c)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Playtime-sketch/Project%20-%20Sketch/src/ReinforcementLearning.ipynb#ch0000018?line=8'>9</a>\u001b[0m model \u001b[39m=\u001b[39m A2C(\u001b[39m\"\u001b[39;49m\u001b[39mMlpPolicy\u001b[39;49m\u001b[39m\"\u001b[39;49m, env, verbose\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Playtime-sketch/Project%20-%20Sketch/src/ReinforcementLearning.ipynb#ch0000018?line=9'>10</a>\u001b[0m model\u001b[39m.\u001b[39mlearn(total_timesteps\u001b[39m=\u001b[39m\u001b[39m25000\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Playtime-sketch/Project%20-%20Sketch/src/ReinforcementLearning.ipynb#ch0000018?line=10'>11</a>\u001b[0m model\u001b[39m.\u001b[39msave(\u001b[39m\"\u001b[39m\u001b[39ma2c_cartpole\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\stable_baselines3\\a2c\\a2c.py:79\u001b[0m, in \u001b[0;36mA2C.__init__\u001b[1;34m(self, policy, env, learning_rate, n_steps, gamma, gae_lambda, ent_coef, vf_coef, max_grad_norm, rms_prop_eps, use_rms_prop, use_sde, sde_sample_freq, normalize_advantage, tensorboard_log, create_eval_env, policy_kwargs, verbose, seed, device, _init_setup_model)\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\n\u001b[0;32m     55\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m     56\u001b[0m     policy: Union[\u001b[39mstr\u001b[39m, Type[ActorCriticPolicy]],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     76\u001b[0m     _init_setup_model: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m     77\u001b[0m ):\n\u001b[1;32m---> 79\u001b[0m     \u001b[39msuper\u001b[39;49m(A2C, \u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(\n\u001b[0;32m     80\u001b[0m         policy,\n\u001b[0;32m     81\u001b[0m         env,\n\u001b[0;32m     82\u001b[0m         learning_rate\u001b[39m=\u001b[39;49mlearning_rate,\n\u001b[0;32m     83\u001b[0m         n_steps\u001b[39m=\u001b[39;49mn_steps,\n\u001b[0;32m     84\u001b[0m         gamma\u001b[39m=\u001b[39;49mgamma,\n\u001b[0;32m     85\u001b[0m         gae_lambda\u001b[39m=\u001b[39;49mgae_lambda,\n\u001b[0;32m     86\u001b[0m         ent_coef\u001b[39m=\u001b[39;49ment_coef,\n\u001b[0;32m     87\u001b[0m         vf_coef\u001b[39m=\u001b[39;49mvf_coef,\n\u001b[0;32m     88\u001b[0m         max_grad_norm\u001b[39m=\u001b[39;49mmax_grad_norm,\n\u001b[0;32m     89\u001b[0m         use_sde\u001b[39m=\u001b[39;49muse_sde,\n\u001b[0;32m     90\u001b[0m         sde_sample_freq\u001b[39m=\u001b[39;49msde_sample_freq,\n\u001b[0;32m     91\u001b[0m         tensorboard_log\u001b[39m=\u001b[39;49mtensorboard_log,\n\u001b[0;32m     92\u001b[0m         policy_kwargs\u001b[39m=\u001b[39;49mpolicy_kwargs,\n\u001b[0;32m     93\u001b[0m         verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[0;32m     94\u001b[0m         device\u001b[39m=\u001b[39;49mdevice,\n\u001b[0;32m     95\u001b[0m         create_eval_env\u001b[39m=\u001b[39;49mcreate_eval_env,\n\u001b[0;32m     96\u001b[0m         seed\u001b[39m=\u001b[39;49mseed,\n\u001b[0;32m     97\u001b[0m         _init_setup_model\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m     98\u001b[0m         supported_action_spaces\u001b[39m=\u001b[39;49m(\n\u001b[0;32m     99\u001b[0m             spaces\u001b[39m.\u001b[39;49mBox,\n\u001b[0;32m    100\u001b[0m             spaces\u001b[39m.\u001b[39;49mDiscrete,\n\u001b[0;32m    101\u001b[0m             spaces\u001b[39m.\u001b[39;49mMultiDiscrete,\n\u001b[0;32m    102\u001b[0m             spaces\u001b[39m.\u001b[39;49mMultiBinary,\n\u001b[0;32m    103\u001b[0m         ),\n\u001b[0;32m    104\u001b[0m     )\n\u001b[0;32m    106\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnormalize_advantage \u001b[39m=\u001b[39m normalize_advantage\n\u001b[0;32m    108\u001b[0m     \u001b[39m# Update optimizer inside the policy if we want to use RMSProp\u001b[39;00m\n\u001b[0;32m    109\u001b[0m     \u001b[39m# (original implementation) rather than Adam\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py:77\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.__init__\u001b[1;34m(self, policy, env, learning_rate, n_steps, gamma, gae_lambda, ent_coef, vf_coef, max_grad_norm, use_sde, sde_sample_freq, policy_base, tensorboard_log, create_eval_env, monitor_wrapper, policy_kwargs, verbose, seed, device, _init_setup_model, supported_action_spaces)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\n\u001b[0;32m     53\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m     54\u001b[0m     policy: Union[\u001b[39mstr\u001b[39m, Type[ActorCriticPolicy]],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     74\u001b[0m     supported_action_spaces: Optional[Tuple[gym\u001b[39m.\u001b[39mspaces\u001b[39m.\u001b[39mSpace, \u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m     75\u001b[0m ):\n\u001b[1;32m---> 77\u001b[0m     \u001b[39msuper\u001b[39;49m(OnPolicyAlgorithm, \u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(\n\u001b[0;32m     78\u001b[0m         policy\u001b[39m=\u001b[39;49mpolicy,\n\u001b[0;32m     79\u001b[0m         env\u001b[39m=\u001b[39;49menv,\n\u001b[0;32m     80\u001b[0m         policy_base\u001b[39m=\u001b[39;49mpolicy_base,\n\u001b[0;32m     81\u001b[0m         learning_rate\u001b[39m=\u001b[39;49mlearning_rate,\n\u001b[0;32m     82\u001b[0m         policy_kwargs\u001b[39m=\u001b[39;49mpolicy_kwargs,\n\u001b[0;32m     83\u001b[0m         verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[0;32m     84\u001b[0m         device\u001b[39m=\u001b[39;49mdevice,\n\u001b[0;32m     85\u001b[0m         use_sde\u001b[39m=\u001b[39;49muse_sde,\n\u001b[0;32m     86\u001b[0m         sde_sample_freq\u001b[39m=\u001b[39;49msde_sample_freq,\n\u001b[0;32m     87\u001b[0m         create_eval_env\u001b[39m=\u001b[39;49mcreate_eval_env,\n\u001b[0;32m     88\u001b[0m         support_multi_env\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m     89\u001b[0m         seed\u001b[39m=\u001b[39;49mseed,\n\u001b[0;32m     90\u001b[0m         tensorboard_log\u001b[39m=\u001b[39;49mtensorboard_log,\n\u001b[0;32m     91\u001b[0m         supported_action_spaces\u001b[39m=\u001b[39;49msupported_action_spaces,\n\u001b[0;32m     92\u001b[0m     )\n\u001b[0;32m     94\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_steps \u001b[39m=\u001b[39m n_steps\n\u001b[0;32m     95\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgamma \u001b[39m=\u001b[39m gamma\n",
      "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\stable_baselines3\\common\\base_class.py:171\u001b[0m, in \u001b[0;36mBaseAlgorithm.__init__\u001b[1;34m(self, policy, env, policy_base, learning_rate, policy_kwargs, tensorboard_log, verbose, device, support_multi_env, create_eval_env, monitor_wrapper, seed, use_sde, sde_sample_freq, supported_action_spaces)\u001b[0m\n\u001b[0;32m    168\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv \u001b[39m=\u001b[39m env\n\u001b[0;32m    170\u001b[0m \u001b[39mif\u001b[39;00m supported_action_spaces \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 171\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39maction_space, supported_action_spaces), (\n\u001b[0;32m    172\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mThe algorithm only supports \u001b[39m\u001b[39m{\u001b[39;00msupported_action_spaces\u001b[39m}\u001b[39;00m\u001b[39m as action spaces \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    173\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbut \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39maction_space\u001b[39m}\u001b[39;00m\u001b[39m was provided\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    174\u001b[0m     )\n\u001b[0;32m    176\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m support_multi_env \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_envs \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m    177\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    178\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mError: the model does not support multiple envs; it requires \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m\"\u001b[39m\u001b[39ma single vectorized environment.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    179\u001b[0m     )\n",
      "\u001b[1;31mAssertionError\u001b[0m: The algorithm only supports (<class 'gym.spaces.box.Box'>, <class 'gym.spaces.discrete.Discrete'>, <class 'gym.spaces.multi_discrete.MultiDiscrete'>, <class 'gym.spaces.multi_binary.MultiBinary'>) as action spaces but Dict(altitude: Discrete(29700, start=300), distance: Discrete(30, start=15), gap: Discrete(5, start=1), length: Discrete(30, start=15), maneuver: Discrete(4), radius: Discrete(3, start=2), speed: Discrete(100, start=100), width: Discrete(30, start=15)) was provided"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action(state):\n",
    "    global steps_done\n",
    "    sample = random.random()\n",
    "    eps_threshold = EPS_END + (EPS_START - EPS_END) * \\\n",
    "        math.exp(-1. * steps_done / EPS_DECAY)\n",
    "    steps_done += 1\n",
    "    if sample > eps_threshold:\n",
    "        with torch.no_grad():\n",
    "            # t.max(1) will return largest column value of each row.\n",
    "            # second column on max result is index of where max element was\n",
    "            # found, so we pick action with the larger expected reward.\n",
    "            st = torch.tensor(state, dtype=torch.float, device=device)\n",
    "            print(\"St =\", st)\n",
    "            print(\"Selection action \", policy_net(st))\n",
    "            return policy_net(torch.tensor(state,\n",
    "                              dtype=torch.float, device=device)\n",
    "                              ).max(1)[1].view(1, 1)\n",
    "    else:\n",
    "        return torch.tensor([[value.sample() for \n",
    "                            value in env.action_space.values()]], \n",
    "                            device=device, dtype=torch.long)\n",
    "\n",
    "\n",
    "episode_durations = []\n",
    "\n",
    "\n",
    "def plot_durations():\n",
    "    plt.figure(2)\n",
    "    plt.clf()\n",
    "    durations_t = torch.tensor(episode_durations, dtype=torch.float)\n",
    "    plt.title('Training...')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Duration')\n",
    "    plt.plot(durations_t.numpy())\n",
    "    # Take 100 episode averages and plot them too\n",
    "    if len(durations_t) >= 100:\n",
    "        means = durations_t.unfold(0, 100, 1).mean(1).view(-1)\n",
    "        means = torch.cat((torch.zeros(99), means))\n",
    "        plt.plot(means.numpy())\n",
    "\n",
    "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
    "    if is_ipython:\n",
    "        display.clear_output(wait=True)\n",
    "        display.display(plt.gcf())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_model():\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return\n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "    # Transpose the batch (see https://stackoverflow.com/a/19343/3343043 for\n",
    "    # detailed explanation). This converts batch-array of Transitions\n",
    "    # to Transition of batch-arrays.\n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "    # Compute a mask of non-final states and concatenate the batch elements\n",
    "    # (a final state would've been the one after which simulation ended)\n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
    "                                          batch.next_state)), device=device, dtype=torch.bool)\n",
    "    non_final_next_states = torch.cat([s for s in batch.next_state\n",
    "                                                if s is not None])\n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "\n",
    "    # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n",
    "    # columns of actions taken. These are the actions which would've been taken\n",
    "    # for each batch state according to policy_net\n",
    "    state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
    "\n",
    "    # Compute V(s_{t+1}) for all next states.\n",
    "    # Expected values of actions for non_final_next_states are computed based\n",
    "    # on the \"older\" target_net; selecting their best reward with max(1)[0].\n",
    "    # This is merged based on the mask, such that we'll have either the expected\n",
    "    # state value or 0 in case the state was final.\n",
    "    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
    "    next_state_values[non_final_mask] = target_net(non_final_next_states).max(1)[0].detach()\n",
    "    # Compute the expected Q values\n",
    "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "\n",
    "    # Compute Huber loss\n",
    "    criterion = nn.SmoothL1Loss()\n",
    "    loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "    # Optimize the model\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    for param in policy_net.parameters():\n",
    "        param.grad.data.clamp_(-1, 1)\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RESET ENV\n",
      "['altitude', 'distance', 'gap', 'length', 'maneuver', 'radius', 'speed', 'width'] [2653, 41, 2, 31, 0, 3, 105, 31]\n",
      "Action in step : {'altitude': 2653, 'distance': 41, 'gap': 2, 'length': 31, 'maneuver': 0, 'radius': 3, 'speed': 105, 'width': 31}\n",
      "['altitude', 'distance', 'gap', 'length', 'maneuver', 'radius', 'speed', 'width'] [2261, 30, 3, 37, 2, 2, 172, 21]\n",
      "Action in step : {'altitude': 2261, 'distance': 30, 'gap': 3, 'length': 37, 'maneuver': 2, 'radius': 2, 'speed': 172, 'width': 21}\n",
      "(303.77782506686026, 1.4326913747893374e-08)\n",
      "['altitude', 'distance', 'gap', 'length', 'maneuver', 'radius', 'speed', 'width'] [2120, 27, 4, 41, 1, 3, 143, 34]\n",
      "Action in step : {'altitude': 2120, 'distance': 27, 'gap': 4, 'length': 41, 'maneuver': 1, 'radius': 3, 'speed': 143, 'width': 34}\n",
      "['altitude', 'distance', 'gap', 'length', 'maneuver', 'radius', 'speed', 'width'] [17156, 43, 4, 29, 1, 4, 193, 17]\n",
      "Action in step : {'altitude': 17156, 'distance': 43, 'gap': 4, 'length': 29, 'maneuver': 1, 'radius': 4, 'speed': 193, 'width': 17}\n",
      "['altitude', 'distance', 'gap', 'length', 'maneuver', 'radius', 'speed', 'width'] [28234, 43, 3, 30, 0, 3, 163, 24]\n",
      "Action in step : {'altitude': 28234, 'distance': 43, 'gap': 3, 'length': 30, 'maneuver': 0, 'radius': 3, 'speed': 163, 'width': 24}\n",
      "['altitude', 'distance', 'gap', 'length', 'maneuver', 'radius', 'speed', 'width'] [3572, 38, 1, 44, 2, 2, 106, 29]\n",
      "Action in step : {'altitude': 3572, 'distance': 38, 'gap': 1, 'length': 44, 'maneuver': 2, 'radius': 2, 'speed': 106, 'width': 29}\n",
      "(1452.686553807879, 2.799934814508786e-06)\n",
      "['altitude', 'distance', 'gap', 'length', 'maneuver', 'radius', 'speed', 'width'] [8573, 28, 2, 21, 0, 3, 198, 38]\n",
      "Action in step : {'altitude': 8573, 'distance': 28, 'gap': 2, 'length': 21, 'maneuver': 0, 'radius': 3, 'speed': 198, 'width': 38}\n",
      "['altitude', 'distance', 'gap', 'length', 'maneuver', 'radius', 'speed', 'width'] [15541, 32, 5, 43, 0, 4, 123, 28]\n",
      "Action in step : {'altitude': 15541, 'distance': 32, 'gap': 5, 'length': 43, 'maneuver': 0, 'radius': 4, 'speed': 123, 'width': 28}\n",
      "['altitude', 'distance', 'gap', 'length', 'maneuver', 'radius', 'speed', 'width'] [17681, 34, 3, 29, 2, 3, 148, 19]\n",
      "Action in step : {'altitude': 17681, 'distance': 34, 'gap': 3, 'length': 29, 'maneuver': 2, 'radius': 3, 'speed': 148, 'width': 19}\n",
      "(178.05009231984434, 2.431189550239326e-07)\n",
      "['altitude', 'distance', 'gap', 'length', 'maneuver', 'radius', 'speed', 'width'] [20646, 27, 5, 29, 0, 3, 101, 22]\n",
      "Action in step : {'altitude': 20646, 'distance': 27, 'gap': 5, 'length': 29, 'maneuver': 0, 'radius': 3, 'speed': 101, 'width': 22}\n",
      "['altitude', 'distance', 'gap', 'length', 'maneuver', 'radius', 'speed', 'width'] [17603, 28, 5, 16, 2, 3, 169, 22]\n",
      "Action in step : {'altitude': 17603, 'distance': 28, 'gap': 5, 'length': 16, 'maneuver': 2, 'radius': 3, 'speed': 169, 'width': 22}\n",
      "(20.251600274792867, 3.3163188467402347e-10)\n",
      "['altitude', 'distance', 'gap', 'length', 'maneuver', 'radius', 'speed', 'width'] [28778, 18, 5, 33, 3, 4, 124, 32]\n",
      "Action in step : {'altitude': 28778, 'distance': 18, 'gap': 5, 'length': 33, 'maneuver': 3, 'radius': 4, 'speed': 124, 'width': 32}\n",
      "['altitude', 'distance', 'gap', 'length', 'maneuver', 'radius', 'speed', 'width'] [6698, 40, 2, 38, 3, 2, 154, 39]\n",
      "Action in step : {'altitude': 6698, 'distance': 40, 'gap': 2, 'length': 38, 'maneuver': 3, 'radius': 2, 'speed': 154, 'width': 39}\n",
      "['altitude', 'distance', 'gap', 'length', 'maneuver', 'radius', 'speed', 'width'] [9268, 20, 4, 16, 1, 2, 173, 15]\n",
      "Action in step : {'altitude': 9268, 'distance': 20, 'gap': 4, 'length': 16, 'maneuver': 1, 'radius': 2, 'speed': 173, 'width': 15}\n",
      "['altitude', 'distance', 'gap', 'length', 'maneuver', 'radius', 'speed', 'width'] [1919, 33, 1, 39, 0, 2, 134, 19]\n",
      "Action in step : {'altitude': 1919, 'distance': 33, 'gap': 1, 'length': 39, 'maneuver': 0, 'radius': 2, 'speed': 134, 'width': 19}\n",
      "['altitude', 'distance', 'gap', 'length', 'maneuver', 'radius', 'speed', 'width'] [2628, 37, 5, 42, 2, 4, 109, 44]\n",
      "Action in step : {'altitude': 2628, 'distance': 37, 'gap': 5, 'length': 42, 'maneuver': 2, 'radius': 4, 'speed': 109, 'width': 44}\n",
      "(216.7686844384103, 1.6136746777251998e-07)\n",
      "St = tensor([[  12.3600, 3172.7300]])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Given groups=1, weight of size [16, 2, 5], expected input[1, 1, 2] to have 2 channels, but got 1 channels instead",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32md:\\Playtime-sketch\\Project - Sketch\\src\\ReinforcementLearning.ipynb Cellule 7\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Playtime-sketch/Project%20-%20Sketch/src/ReinforcementLearning.ipynb#ch0000006?line=3'>4</a>\u001b[0m state \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mreset()\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Playtime-sketch/Project%20-%20Sketch/src/ReinforcementLearning.ipynb#ch0000006?line=4'>5</a>\u001b[0m \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m count():\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Playtime-sketch/Project%20-%20Sketch/src/ReinforcementLearning.ipynb#ch0000006?line=5'>6</a>\u001b[0m     \u001b[39m# Select and perform an action\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Playtime-sketch/Project%20-%20Sketch/src/ReinforcementLearning.ipynb#ch0000006?line=6'>7</a>\u001b[0m     action \u001b[39m=\u001b[39m select_action((\u001b[39mlist\u001b[39;49m(state\u001b[39m.\u001b[39;49mvalues()),))\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Playtime-sketch/Project%20-%20Sketch/src/ReinforcementLearning.ipynb#ch0000006?line=7'>8</a>\u001b[0m     action \u001b[39m=\u001b[39m action\u001b[39m.\u001b[39mtolist()[\u001b[39m0\u001b[39m]\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Playtime-sketch/Project%20-%20Sketch/src/ReinforcementLearning.ipynb#ch0000006?line=8'>9</a>\u001b[0m     \u001b[39mprint\u001b[39m(ordered_keys, action)\n",
      "\u001b[1;32md:\\Playtime-sketch\\Project - Sketch\\src\\ReinforcementLearning.ipynb Cellule 7\u001b[0m in \u001b[0;36mselect_action\u001b[1;34m(state)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Playtime-sketch/Project%20-%20Sketch/src/ReinforcementLearning.ipynb#ch0000006?line=11'>12</a>\u001b[0m         st \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(state, dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mfloat, device\u001b[39m=\u001b[39mdevice)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Playtime-sketch/Project%20-%20Sketch/src/ReinforcementLearning.ipynb#ch0000006?line=12'>13</a>\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mSt =\u001b[39m\u001b[39m\"\u001b[39m, st)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/Playtime-sketch/Project%20-%20Sketch/src/ReinforcementLearning.ipynb#ch0000006?line=13'>14</a>\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mSelection action \u001b[39m\u001b[39m\"\u001b[39m, policy_net(st))\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Playtime-sketch/Project%20-%20Sketch/src/ReinforcementLearning.ipynb#ch0000006?line=14'>15</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m policy_net(torch\u001b[39m.\u001b[39mtensor(state,\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Playtime-sketch/Project%20-%20Sketch/src/ReinforcementLearning.ipynb#ch0000006?line=15'>16</a>\u001b[0m                           dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mfloat, device\u001b[39m=\u001b[39mdevice)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Playtime-sketch/Project%20-%20Sketch/src/ReinforcementLearning.ipynb#ch0000006?line=16'>17</a>\u001b[0m                           )\u001b[39m.\u001b[39mmax(\u001b[39m1\u001b[39m)[\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mview(\u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Playtime-sketch/Project%20-%20Sketch/src/ReinforcementLearning.ipynb#ch0000006?line=17'>18</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32md:\\Playtime-sketch\\Project - Sketch\\src\\gymenv\\action_manager.py:40\u001b[0m, in \u001b[0;36mActionModel.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m---> 40\u001b[0m     x \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconv1(x))\n\u001b[0;32m     41\u001b[0m     x \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv1(x))\n\u001b[0;32m     42\u001b[0m     x \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv1(x))\n",
      "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\nn\\modules\\conv.py:307\u001b[0m, in \u001b[0;36mConv1d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    306\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 307\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\nn\\modules\\conv.py:303\u001b[0m, in \u001b[0;36mConv1d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    299\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m    300\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv1d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[0;32m    301\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[0;32m    302\u001b[0m                     _single(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[1;32m--> 303\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv1d(\u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[0;32m    304\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Given groups=1, weight of size [16, 2, 5], expected input[1, 1, 2] to have 2 channels, but got 1 channels instead"
     ]
    }
   ],
   "source": [
    "num_episodes = 50\n",
    "for i_episode in range(num_episodes):\n",
    "    # Initialize the environment and state\n",
    "    state = env.reset()\n",
    "    for t in count():\n",
    "        # Select and perform an action\n",
    "        action = select_action((list(state.values()),))\n",
    "        action = action.tolist()[0]\n",
    "        print(ordered_keys, action)\n",
    "        action = dict(zip(ordered_keys, action))\n",
    "        last_state = state\n",
    "        obs, reward, done, _ = env.step(action)\n",
    "        reward = torch.tensor([reward], device=device)\n",
    "\n",
    "        # Observe new state\n",
    "        \n",
    "        current_state = obs\n",
    "        if not done:\n",
    "            next_state = dict()\n",
    "            for key in current_state.keys():\n",
    "                next_state[key] = current_state[key] - last_state[key]\n",
    "        else:\n",
    "            next_state = None\n",
    "\n",
    "        # Store the transition in memory\n",
    "        memory.push(state, action, next_state, reward)\n",
    "\n",
    "        # Move to the next state\n",
    "        state = next_state\n",
    "\n",
    "        # Perform one step of the optimization (on the policy network)\n",
    "        optimize_model()\n",
    "        if done:\n",
    "            episode_durations.append(t + 1)\n",
    "            plot_durations()\n",
    "            break\n",
    "    # Update the target network, copying all weights and biases in DQN\n",
    "    if i_episode % TARGET_UPDATE == 0:\n",
    "        target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "print('Complete')\n",
    "env.render()\n",
    "env.close()\n",
    "\n",
    "plt.ioff()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's try to add an agent with a deep learning neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(env.observation_space)\n",
    "states = (len(env.observation_space.spaces.keys()),)\n",
    "actions = env.action_space.n \n",
    "num_hidden = 24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(states, actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EntrÃ©e : paramÃ¨tres de wheel\n",
    "# Ajouter aussi les choix de plusieurs manoeuvres\n",
    "\n",
    "inputs = Input(shape=states)\n",
    "first = Dense(num_hidden, activation='relu')(inputs)\n",
    "second = Dense(num_hidden, activation='relu')(first)\n",
    "out_speed = Dense(env.action_space.spaces['speed'].n, activation='linear')(second)\n",
    "out_altitude = Dense(env.action_space.spaces['altitude'].n, activation='linear')(second)\n",
    "out_distance = Dense(env.action_space.spaces['distance'].n, activation='linear')(second)\n",
    "\n",
    "model = Model(inputs=inputs, outputs=[out_speed, out_altitude, out_distance])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rl.agents import DDPGAgent, DQNAgent\n",
    "from rl.policy import BoltzmannQPolicy \n",
    "from rl.memory import SequentialMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_agent(model, actions):\n",
    "    policy = BoltzmannQPolicy()\n",
    "    memory = SequentialMemory(limit=50000, window_length=1)\n",
    "    dqn = DQNAgent(model=model, memory=memory, policy=policy, \n",
    "                   nb_actions=actions, nb_steps_warmup=10, target_model_update=1e-2)\n",
    "    return dqn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn = build_agent(states, actions)\n",
    "dqn.compile(Adam(lr=1e-3), metrics=['mae'])\n",
    "dqn.fit(env, nb_steps=50000, visualize=False, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f544ce1a915a9875fad91c894e2c0bcad4b7a79945aa6027ef3ad27810072aa6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
