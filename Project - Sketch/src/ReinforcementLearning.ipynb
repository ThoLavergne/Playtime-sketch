{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cet avion peut voler pendant  52438  s, soit  873  min, soit 2039  km Ã  une vitesse moyenne de  140  km/h et sa vitesse max sera de  200  km/h\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Roaming\\Python\\Python38\\site-packages\\seaborn\\rcmod.py:82: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if LooseVersion(mpl.__version__) >= \"3.0\":\n",
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\setuptools\\_distutils\\version.py:346: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  other = LooseVersion(other)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.pyplot._IonContext at 0x21c0b3bda30>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import count\n",
    "from gymenv.PlaytimeEnv import PlaytimeEnv\n",
    "from gymenv.action_manager import ActionModel\n",
    "from objects.Maneuver import Mission_Maneuver\n",
    "from objects.AllManeuvers import LIST_MAN\n",
    "from objects.Plane import ULM\n",
    "from function.tools import *\n",
    "from function.j_methods import * \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from collections import namedtuple, deque\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "plt.ion()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Plane': ULM, 'GoalDistance': 0, 'RtBDistance': 0, 'Fuel': 3, 'Meteo': 'Sunny', 'MissionType': SCAR, 'Strength': 'Weak', 'TimeMin': 300, 'SynchroTime': 0}\n"
     ]
    }
   ],
   "source": [
    "param_list = {\n",
    "    'Plane': [ULM],\n",
    "    'GoalDistance': [0, 10, 15, 20, 30],\n",
    "    'RtBDistance': [0, 10, 15, 20, 30],\n",
    "    'Fuel': [1, 2, 3, 3, 4, 10],\n",
    "    # Start with just 2 states : good (sunny, no clouds) and bad (cloudy or rainy)\n",
    "    'Meteo': [\"Sunny\", \"Cloudy\", \"Misty\"],\n",
    "    'MissionType': [Mission_Maneuver.SCAR],  # , Mission_Maneuver.CAS\n",
    "    # Add ennemies number afterwards, weaponry\n",
    "    'Strength': ['Weak', 'Equal', 'Strong'],\n",
    "    'TimeMin': [0, 300],\n",
    "    'SynchroTime': [0, 1000],\n",
    "}\n",
    "combinations = get_all_combinations(param_list)\n",
    "first_c = list(filter(lambda c : c['TimeMin'] == 300 and c['Fuel'] == 3, combinations))[0]\n",
    "print(first_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([],maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Save a transition\"\"\"\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('altitude', 6299), ('distance', 42), ('gap', 2), ('length', 32), ('maneuver', 3), ('speed', 181), ('width', 31)])\n",
      "[12033, 29, 2, 24, 3, 139, 28]\n",
      "4\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "env = PlaytimeEnv(combinations)\n",
    "env.reset()\n",
    "a = env.action_space.sample()\n",
    "print(a)\n",
    "print([value.sample() for value in env.action_space.values()])\n",
    "nb_param_out = max(man._nb_param_() for man in LIST_MAN) + 1\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "GAMMA = 0.999\n",
    "EPS_START = 0.9\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY = 200\n",
    "TARGET_UPDATE = 10\n",
    "\n",
    "policy_net = ActionModel(len(env.state), nb_param_out)\n",
    "target_net  = ActionModel(len(env.state), nb_param_out)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()\n",
    "\n",
    "optimizer = optim.RMSprop(policy_net.parameters())\n",
    "memory = ReplayMemory(10000)\n",
    "steps_done = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action(state):\n",
    "    global steps_done\n",
    "    sample = random.random()\n",
    "    eps_threshold = EPS_END + (EPS_START - EPS_END) * \\\n",
    "        math.exp(-1. * steps_done / EPS_DECAY)\n",
    "    steps_done += 1\n",
    "    if sample > eps_threshold:\n",
    "        with torch.no_grad():\n",
    "            # t.max(1) will return largest column value of each row.\n",
    "            # second column on max result is index of where max element was\n",
    "            # found, so we pick action with the larger expected reward.\n",
    "            return policy_net(state).max(1)[1].view(1, 1)\n",
    "    else:\n",
    "        return torch.tensor([[value.sample() for \n",
    "                            value in env.action_space.values()]], \n",
    "                            device=device, dtype=torch.long)\n",
    "\n",
    "\n",
    "episode_durations = []\n",
    "\n",
    "\n",
    "def plot_durations():\n",
    "    plt.figure(2)\n",
    "    plt.clf()\n",
    "    durations_t = torch.tensor(episode_durations, dtype=torch.float)\n",
    "    plt.title('Training...')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Duration')\n",
    "    plt.plot(durations_t.numpy())\n",
    "    # Take 100 episode averages and plot them too\n",
    "    if len(durations_t) >= 100:\n",
    "        means = durations_t.unfold(0, 100, 1).mean(1).view(-1)\n",
    "        means = torch.cat((torch.zeros(99), means))\n",
    "        plt.plot(means.numpy())\n",
    "\n",
    "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
    "    if is_ipython:\n",
    "        display.clear_output(wait=True)\n",
    "        display.display(plt.gcf())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_model():\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return\n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "    # Transpose the batch (see https://stackoverflow.com/a/19343/3343043 for\n",
    "    # detailed explanation). This converts batch-array of Transitions\n",
    "    # to Transition of batch-arrays.\n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "    # Compute a mask of non-final states and concatenate the batch elements\n",
    "    # (a final state would've been the one after which simulation ended)\n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
    "                                          batch.next_state)), device=device, dtype=torch.bool)\n",
    "    non_final_next_states = torch.cat([s for s in batch.next_state\n",
    "                                                if s is not None])\n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "\n",
    "    # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n",
    "    # columns of actions taken. These are the actions which would've been taken\n",
    "    # for each batch state according to policy_net\n",
    "    state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
    "\n",
    "    # Compute V(s_{t+1}) for all next states.\n",
    "    # Expected values of actions for non_final_next_states are computed based\n",
    "    # on the \"older\" target_net; selecting their best reward with max(1)[0].\n",
    "    # This is merged based on the mask, such that we'll have either the expected\n",
    "    # state value or 0 in case the state was final.\n",
    "    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
    "    next_state_values[non_final_mask] = target_net(non_final_next_states).max(1)[0].detach()\n",
    "    # Compute the expected Q values\n",
    "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "\n",
    "    # Compute Huber loss\n",
    "    criterion = nn.SmoothL1Loss()\n",
    "    loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "    # Optimize the model\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    for param in policy_net.parameters():\n",
    "        param.grad.data.clamp_(-1, 1)\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action selected : tensor([[3587,   20,    2,   38,    3,  184,   38]])\n",
      "Action in step : [3587, 20, 2, 38, 3, 184, 38]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "3587",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32md:\\Playtime-sketch\\Project - Sketch\\src\\ReinforcementLearning.ipynb Cell 7'\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Playtime-sketch/Project%20-%20Sketch/src/ReinforcementLearning.ipynb#ch0000006?line=7'>8</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mAction selected :\u001b[39m\u001b[39m\"\u001b[39m, action)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Playtime-sketch/Project%20-%20Sketch/src/ReinforcementLearning.ipynb#ch0000006?line=8'>9</a>\u001b[0m last_state \u001b[39m=\u001b[39m state\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/Playtime-sketch/Project%20-%20Sketch/src/ReinforcementLearning.ipynb#ch0000006?line=9'>10</a>\u001b[0m obs, reward, done, _ \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39;49mstep(action\u001b[39m.\u001b[39;49mtolist())\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Playtime-sketch/Project%20-%20Sketch/src/ReinforcementLearning.ipynb#ch0000006?line=10'>11</a>\u001b[0m reward \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor([reward], device\u001b[39m=\u001b[39mdevice)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Playtime-sketch/Project%20-%20Sketch/src/ReinforcementLearning.ipynb#ch0000006?line=12'>13</a>\u001b[0m \u001b[39m# Observe new state\u001b[39;00m\n",
      "File \u001b[1;32md:\\Playtime-sketch\\Project - Sketch\\src\\gymenv\\PlaytimeEnv.py:78\u001b[0m, in \u001b[0;36mPlaytimeEnv.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     76\u001b[0m action \u001b[39m=\u001b[39m action[\u001b[39m0\u001b[39m]\n\u001b[0;32m     77\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mAction in step :\u001b[39m\u001b[39m\"\u001b[39m, action)\n\u001b[1;32m---> 78\u001b[0m maneuver \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmaneuvers_index[action[\u001b[39m0\u001b[39;49m]]\n\u001b[0;32m     79\u001b[0m nb_param \u001b[39m=\u001b[39m maneuver\u001b[39m.\u001b[39m_nb_params_()\n\u001b[0;32m     80\u001b[0m \u001b[39mprint\u001b[39m(nb_param)\n",
      "\u001b[1;31mKeyError\u001b[0m: 3587"
     ]
    }
   ],
   "source": [
    "num_episodes = 50\n",
    "for i_episode in range(num_episodes):\n",
    "    # Initialize the environment and state\n",
    "    state = env.reset()\n",
    "    for t in count():\n",
    "        # Select and perform an action\n",
    "        action = select_action(state)\n",
    "        print(\"Action selected :\", action)\n",
    "        last_state = state\n",
    "        obs, reward, done, _ = env.step(action.tolist())\n",
    "        reward = torch.tensor([reward], device=device)\n",
    "\n",
    "        # Observe new state\n",
    "        \n",
    "        current_state = obs\n",
    "        if not done:\n",
    "            next_state = current_state - last_state\n",
    "        else:\n",
    "            next_state = None\n",
    "\n",
    "        # Store the transition in memory\n",
    "        memory.push(state, action, next_state, reward)\n",
    "\n",
    "        # Move to the next state\n",
    "        state = next_state\n",
    "\n",
    "        # Perform one step of the optimization (on the policy network)\n",
    "        optimize_model()\n",
    "        if done:\n",
    "            episode_durations.append(t + 1)\n",
    "            plot_durations()\n",
    "            break\n",
    "    # Update the target network, copying all weights and biases in DQN\n",
    "    if i_episode % TARGET_UPDATE == 0:\n",
    "        target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "print('Complete')\n",
    "env.render()\n",
    "env.close()\n",
    "plt.ioff()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "5",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32md:\\Playtime-sketch\\Project - Sketch\\src\\ReinforcementLearning.ipynb Cell 6'\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Playtime-sketch/Project%20-%20Sketch/src/ReinforcementLearning.ipynb#ch0000007?line=0'>1</a>\u001b[0m env \u001b[39m=\u001b[39m PlaytimeEnv(first_c)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Playtime-sketch/Project%20-%20Sketch/src/ReinforcementLearning.ipynb#ch0000007?line=2'>3</a>\u001b[0m num_steps \u001b[39m=\u001b[39m \u001b[39m150\u001b[39m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Playtime-sketch/Project%20-%20Sketch/src/ReinforcementLearning.ipynb#ch0000007?line=3'>4</a>\u001b[0m obs \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39;49mreset()\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Playtime-sketch/Project%20-%20Sketch/src/ReinforcementLearning.ipynb#ch0000007?line=4'>5</a>\u001b[0m \u001b[39mfor\u001b[39;00m step \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_steps):\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Playtime-sketch/Project%20-%20Sketch/src/ReinforcementLearning.ipynb#ch0000007?line=5'>6</a>\u001b[0m     \u001b[39m# take random action, but you can also do something more intelligent\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Playtime-sketch/Project%20-%20Sketch/src/ReinforcementLearning.ipynb#ch0000007?line=6'>7</a>\u001b[0m     \u001b[39m# action = my_intelligent_agent_fn(obs) \u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Playtime-sketch/Project%20-%20Sketch/src/ReinforcementLearning.ipynb#ch0000007?line=7'>8</a>\u001b[0m     \u001b[39m# action = policy(obs)\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Playtime-sketch/Project%20-%20Sketch/src/ReinforcementLearning.ipynb#ch0000007?line=8'>9</a>\u001b[0m     action \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39maction_choose(obs)\n",
      "File \u001b[1;32md:\\Playtime-sketch\\Project - Sketch\\src\\gymenv\\PlaytimeEnv.py:99\u001b[0m, in \u001b[0;36mPlaytimeEnv.reset\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     98\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mreset\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m---> 99\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_new_gameplan()\n\u001b[0;32m    100\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreset_action_state()\n\u001b[0;32m    101\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmaneuver_list \u001b[39m=\u001b[39m []\n",
      "File \u001b[1;32md:\\Playtime-sketch\\Project - Sketch\\src\\gymenv\\PlaytimeEnv.py:183\u001b[0m, in \u001b[0;36mPlaytimeEnv.get_new_gameplan\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    181\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_new_gameplan\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    182\u001b[0m     r \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mrandint(\u001b[39m0\u001b[39m, \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgameplan_list))\n\u001b[1;32m--> 183\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgameplan \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgameplan_list[r]\n\u001b[0;32m    184\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mplane \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgameplan[\u001b[39m'\u001b[39m\u001b[39mPlane\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m    185\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgoalDistance \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgameplan[\u001b[39m'\u001b[39m\u001b[39mGoalDistance\u001b[39m\u001b[39m'\u001b[39m]\n",
      "\u001b[1;31mKeyError\u001b[0m: 5"
     ]
    }
   ],
   "source": [
    "env = PlaytimeEnv(first_c)\n",
    "\n",
    "num_steps = 150\n",
    "obs = env.reset()\n",
    "for step in range(num_steps):\n",
    "    # take random action, but you can also do something more intelligent\n",
    "    # action = my_intelligent_agent_fn(obs) \n",
    "    # action = policy(obs)\n",
    "    action = env.action_choose(obs)\n",
    "    # apply the action\n",
    "    obs, reward, done, info = env.step(action)\n",
    "    print(obs, reward, done, info)\n",
    "    # Render the env\n",
    "    # env.render()\n",
    "\n",
    "    # Wait a bit before the next frame unless you want to see a crazy fast video\n",
    "    time.sleep(0.001)\n",
    "    \n",
    "    # If the episode is up, then start another one\n",
    "    if done:\n",
    "        print(\"Reset now\")\n",
    "        env.reset()\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's try to add an agent with a deep learning neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Box(2,)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Box' object has no attribute 'spaces'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32md:\\Playtime-sketch\\Project - Sketch\\src\\ReinforcementLearning.ipynb Cell 11'\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Playtime-sketch/Project%20-%20Sketch/src/ReinforcementLearning.ipynb#ch0000010?line=0'>1</a>\u001b[0m \u001b[39mprint\u001b[39m(env\u001b[39m.\u001b[39mobservation_space)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Playtime-sketch/Project%20-%20Sketch/src/ReinforcementLearning.ipynb#ch0000010?line=1'>2</a>\u001b[0m states \u001b[39m=\u001b[39m (\u001b[39mlen\u001b[39m(env\u001b[39m.\u001b[39;49mobservation_space\u001b[39m.\u001b[39;49mspaces\u001b[39m.\u001b[39mkeys()),)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Playtime-sketch/Project%20-%20Sketch/src/ReinforcementLearning.ipynb#ch0000010?line=2'>3</a>\u001b[0m actions \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39maction_space\u001b[39m.\u001b[39mn \n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Playtime-sketch/Project%20-%20Sketch/src/ReinforcementLearning.ipynb#ch0000010?line=3'>4</a>\u001b[0m num_hidden \u001b[39m=\u001b[39m \u001b[39m24\u001b[39m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Box' object has no attribute 'spaces'"
     ]
    }
   ],
   "source": [
    "print(env.observation_space)\n",
    "states = (len(env.observation_space.spaces.keys()),)\n",
    "actions = env.action_space.n \n",
    "num_hidden = 24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2,) 3\n"
     ]
    }
   ],
   "source": [
    "print(states, actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EntrÃ©e : paramÃ¨tres de wheel\n",
    "# Ajouter aussi les choix de plusieurs manoeuvres\n",
    "\n",
    "inputs = Input(shape=states)\n",
    "first = Dense(num_hidden, activation='relu')(inputs)\n",
    "second = Dense(num_hidden, activation='relu')(first)\n",
    "out_speed = Dense(env.action_space.spaces['speed'].n, activation='linear')(second)\n",
    "out_altitude = Dense(env.action_space.spaces['altitude'].n, activation='linear')(second)\n",
    "out_distance = Dense(env.action_space.spaces['distance'].n, activation='linear')(second)\n",
    "\n",
    "model = Model(inputs=inputs, outputs=[out_speed, out_altitude, out_distance])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 2)]          0           []                               \n",
      "                                                                                                  \n",
      " dense_8 (Dense)                (None, 24)           72          ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " dense_9 (Dense)                (None, 24)           600         ['dense_8[0][0]']                \n",
      "                                                                                                  \n",
      " dense_10 (Dense)               (None, 10)           250         ['dense_9[0][0]']                \n",
      "                                                                                                  \n",
      " dense_11 (Dense)               (None, 50)           1250        ['dense_9[0][0]']                \n",
      "                                                                                                  \n",
      " dense_12 (Dense)               (None, 6)            150         ['dense_9[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,322\n",
      "Trainable params: 2,322\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rl.agents import DDPGAgent, DQNAgent\n",
    "from rl.policy import BoltzmannQPolicy \n",
    "from rl.memory import SequentialMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_agent(model, actions):\n",
    "    policy = BoltzmannQPolicy()\n",
    "    memory = SequentialMemory(limit=50000, window_length=1)\n",
    "    dqn = DQNAgent(model=model, memory=memory, policy=policy, \n",
    "                   nb_actions=actions, nb_steps_warmup=10, target_model_update=1e-2)\n",
    "    return dqn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'output'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32md:\\Playtime-sketch\\Project - Sketch\\src\\ReinforcementLearning.ipynb Cell 18'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Playtime-sketch/Project%20-%20Sketch/src/ReinforcementLearning.ipynb#ch0000019?line=0'>1</a>\u001b[0m dqn \u001b[39m=\u001b[39m build_agent(states, actions)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Playtime-sketch/Project%20-%20Sketch/src/ReinforcementLearning.ipynb#ch0000019?line=1'>2</a>\u001b[0m dqn\u001b[39m.\u001b[39mcompile(Adam(lr\u001b[39m=\u001b[39m\u001b[39m1e-3\u001b[39m), metrics\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39mmae\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Playtime-sketch/Project%20-%20Sketch/src/ReinforcementLearning.ipynb#ch0000019?line=2'>3</a>\u001b[0m dqn\u001b[39m.\u001b[39mfit(env, nb_steps\u001b[39m=\u001b[39m\u001b[39m50000\u001b[39m, visualize\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, verbose\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n",
      "\u001b[1;32md:\\Playtime-sketch\\Project - Sketch\\src\\ReinforcementLearning.ipynb Cell 17'\u001b[0m in \u001b[0;36mbuild_agent\u001b[1;34m(model, actions)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Playtime-sketch/Project%20-%20Sketch/src/ReinforcementLearning.ipynb#ch0000017?line=1'>2</a>\u001b[0m policy \u001b[39m=\u001b[39m BoltzmannQPolicy()\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Playtime-sketch/Project%20-%20Sketch/src/ReinforcementLearning.ipynb#ch0000017?line=2'>3</a>\u001b[0m memory \u001b[39m=\u001b[39m SequentialMemory(limit\u001b[39m=\u001b[39m\u001b[39m50000\u001b[39m, window_length\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Playtime-sketch/Project%20-%20Sketch/src/ReinforcementLearning.ipynb#ch0000017?line=3'>4</a>\u001b[0m dqn \u001b[39m=\u001b[39m DQNAgent(model\u001b[39m=\u001b[39;49mmodel, memory\u001b[39m=\u001b[39;49mmemory, policy\u001b[39m=\u001b[39;49mpolicy, \n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Playtime-sketch/Project%20-%20Sketch/src/ReinforcementLearning.ipynb#ch0000017?line=4'>5</a>\u001b[0m                nb_actions\u001b[39m=\u001b[39;49mactions, nb_steps_warmup\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m, target_model_update\u001b[39m=\u001b[39;49m\u001b[39m1e-2\u001b[39;49m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Playtime-sketch/Project%20-%20Sketch/src/ReinforcementLearning.ipynb#ch0000017?line=5'>6</a>\u001b[0m \u001b[39mreturn\u001b[39;00m dqn\n",
      "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\rl\\agents\\dqn.py:106\u001b[0m, in \u001b[0;36mDQNAgent.__init__\u001b[1;34m(self, model, policy, test_policy, enable_double_dqn, enable_dueling_network, dueling_type, *args, **kwargs)\u001b[0m\n\u001b[0;32m    103\u001b[0m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    105\u001b[0m \u001b[39m# Validate (important) input.\u001b[39;00m\n\u001b[1;32m--> 106\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlist\u001b[39m(model\u001b[39m.\u001b[39;49moutput\u001b[39m.\u001b[39mshape) \u001b[39m!=\u001b[39m \u001b[39mlist\u001b[39m((\u001b[39mNone\u001b[39;00m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnb_actions)):\n\u001b[0;32m    107\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mModel output \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mmodel\u001b[39m.\u001b[39moutput\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m has invalid shape. DQN expects a model that has one dimension for each action, in this case \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnb_actions\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m    109\u001b[0m \u001b[39m# Parameters.\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'output'"
     ]
    }
   ],
   "source": [
    "dqn = build_agent(states, actions)\n",
    "dqn.compile(Adam(lr=1e-3), metrics=['mae'])\n",
    "dqn.fit(env, nb_steps=50000, visualize=False, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f544ce1a915a9875fad91c894e2c0bcad4b7a79945aa6027ef3ad27810072aa6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
